{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAR6FPEWl_QE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# logistic regration assingment"
      ],
      "metadata": {
        "id": "8qlgTpFemcOs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. question\n",
        "\n",
        "Simple Linear Regression (SLR) is a fundamental statistical and machine learning technique used to model and analyze the relationship between two quantitative variables. It characterizes this relationship by fitting a straight line, known as the \"line of best fit,\" to a set of observed data points.\n",
        "\n",
        "* purpose of slr\n",
        "\n",
        "Predictive Modeling: Estimating future or unknown values of the dependent variable based on a specific value of the independent variable (e.g., predicting sales based on advertising spend).\n",
        "\n",
        "Explanatory Analysis: Quantifying the strength and direction (positive or negative) of the association between two variables to understand how one affects the other.\n",
        "\n",
        "Trend Identification: Modeling data over time or across different conditions to identify general underlying patterns.\n"
      ],
      "metadata": {
        "id": "e6_NJ1x_mmCO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. question\n",
        "\n",
        "Simple linear regression relies on key assumptions for valid results: Linearity (straight-line relationship), Independence (observations aren't related), Homoscedasticity (constant error variance), and Normality (errors are normally distributed), with No Multicollinearity (no high correlation between predictors, though less critical for simple regression) being crucial for multiple regression. Meeting these ensures your model accurately represents the data and its predictions are reliable.\n",
        "\n",
        "Linearity: The relationship between the independent (X) and dependent (Y) variables is a straight line.\n",
        "\n",
        "Independence of Errors: Errors (residuals) are not correlated with each other.\n",
        "Check: Often violated in time-series data (autocorrelation); residual plots should show no pattern.\n",
        "\n"
      ],
      "metadata": {
        "id": "vS0SXMbGnjy4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. question\n",
        "\n",
        "The mathematical equation for a simple linear regression model is \\(Y=\\beta _{0}+\\beta _{1}X+\\epsilon \\), representing the relationship where the dependent variable (\\(Y\\)) is predicted by the independent variable (\\(X\\)) plus a random error (\\(\\epsilon \\)), with \\(\\beta _{0}\\) being the y-intercept and \\(\\beta _{1}\\) the slope, showing how much \\(Y\\) changes for a unit change in \\(X\\).\n",
        "\n",
        "In simpler notation often used in introductory contexts, you might see it as \\(y=a+bx+\\epsilon \\), where \\(a\\) is the intercept and \\(b\\) is the slope.\n"
      ],
      "metadata": {
        "id": "iFgWcHGsn_Ry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. question\n",
        "\n",
        "A great real-world example of simple linear regression is predicting house prices based on square footage, where the square footage is the independent variable (X) and the price is the dependent variable (Y). By analyzing data from past home sales, a real estate analyst can find the best-fit straight line showing how much price increases with each added square foot, helping sellers price homes and buyers budget for a property.\n",
        "\n",
        "The Model: A simple linear regression equation (like \\(Price=\\beta _{0}+\\beta _{1}\\times \\text{SquareFootage}\\)) is fitted to the historical sales data.\n",
        "\n",
        "Application: If the model shows that, on average, a house sells for $100 per square foot (\\(\\beta _{1}=100\\)), a 2,000 sq. ft. house would be predicted to cost around $200,000 (plus the base price \\(\\beta _{0}\\))."
      ],
      "metadata": {
        "id": "XQfB0wFCoU3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. five question\n",
        "\n",
        "The Least Squares Method in linear regression finds the \"line of best fit\" for data by minimizing the sum of the squared vertical distances (errors or residuals) between the actual data points and the predicted points on that line, resulting in the equation (y = mx + c) that best represents the relationship between variables. It's a core technique in statistics to model trends, predict outcomes, and understand relationships, calculating the slope (\\(m\\)) and intercept (\\(c\\)) using specific formulas derived from the data\n",
        "\n",
        "* how it work\n",
        "\n",
        "Data Points: You have a set of \\((x,y)\\) data points on a scatter plot.\n",
        "\n",
        "Trial Line: Imagine drawing many possible lines through the data.\n",
        "\n",
        "Errors (Residuals): For each data point, the vertical distance from the point to the line is the error (residual).\n",
        "\n",
        "Squaring Errors: Square each error to make them positive and penalize larger errors more heavily.\n",
        "\n",
        "Minimize Sum: The Least Squares Method finds the unique line where the sum of these squared errors is the smallest possible, making it the best fit.\n",
        "\n"
      ],
      "metadata": {
        "id": "DHqYVdwfon5L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. question\n",
        "\n",
        "Logistic regression predicts categorical outcomes (like yes/no, spam/not spam) using an S-shaped sigmoid curve to output probabilities (0 to 1), while linear regression predicts continuous values (like price, temperature) using a straight line, differing primarily in their output type (categorical vs. continuous), underlying function (sigmoid vs. linear), and primary use case (classification vs. regression)\n",
        "\n",
        "* logistic\n",
        "\n",
        "Purpose: Classification problems (e.g., predicting if a customer will buy a product or not).\n",
        "\n",
        "Function: Uses the Sigmoid (logistic) function, creating an S-shaped curve to map outputs to probabilities.\n",
        "\n",
        "Math: Models the log-odds (logit) of the outcome as a linear combination of predictors, then transforms it.\n",
        "\n",
        "* linear\n",
        "\n",
        "Purpose: Regression problems (e.g., predicting house prices, sales figures)\n",
        "\n",
        "Function: Models a straight-line relationship between variables.\n",
        "\n",
        "Method: Uses Ordinary Least Squares (OLS) to minimize errors.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZLNhGR2DpEz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. seven question\n",
        "\n",
        "Three common regression metrics are Mean Absolute Error (MAE), the average absolute difference between actual and predicted values (easy to interpret); Mean Squared Error (MSE), which squares errors to penalize larger mistakes more; and R-Squared (RÂ²), indicating the proportion of variance in the dependent variable explained by the model (higher is better).\n",
        "\n",
        "* mean absolute arror\n",
        "\n",
        "Description: Calculates the average magnitude of errors in a set of predictions, without considering their direction (positive/negative). It's the average of the absolute differences between the predicted and actual values.\n",
        "\n",
        "\n",
        "* mean sequence error.\n",
        "\n",
        "Description: Computes the average of the squared errors. Squaring the errors makes it more sensitive to large errors (outliers) compared to MAE."
      ],
      "metadata": {
        "id": "vutW0g5qpwqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. question\n",
        "\n",
        "The purpose of R-squared (Coefficient of Determination) in regression analysis is to measure how well your model fits the data, specifically indicating the proportion of the variance in the dependent variable that is predictable from the independent variables, essentially showing the model's \"goodness-of-fit\" on a scale from 0 to 1 (or 0% to 100%). A higher R-squared value (closer to 1) means the model explains more of the variability, suggesting a better fit, while a lower value (closer to 0) means the model explains less, indicating a poorer fit, with 1 signifying a perfect fit and 0 a random model.\n",
        "\n",
        "* Key Functions of R-squared:\n",
        "\n",
        "Can be Misleading: R-squared always increases (or stays the same) when you add more variables, even useless ones, which can lead to overfitting.\n",
        "\n",
        "Indicates Predictive Power: A high R-squared suggests the independent variables collectively do a good job of explaining changes in the dependent variable, making predictions more reliable.\n",
        "\n"
      ],
      "metadata": {
        "id": "ziEuzX9NqOkD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. question\n",
        "\n",
        "To fit a simple linear regression model using scikit-learn and print the slope and intercept, you can follow this Python code example:\n",
        "\n",
        "import numpy as np: Imports the NumPy library for numerical operations, especially creating the data arrays.\n",
        "\n",
        "from sklearn.linear_model import LinearRegression: Imports the LinearRegression class from scikit-learn.\n",
        "\n",
        "X = np.array(...).reshape(-1, 1): Creates the input feature array X and reshapes it into a 2D array, which is a requirement for scikit-learn's fit method.\n",
        "\n",
        "y = np.array(...): Creates the target variable array y.\n",
        "\n",
        "model = LinearRegression(): Initializes the regression model"
      ],
      "metadata": {
        "id": "P-6lUTlLqmlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# 1. Prepare sample data (X = feature, y = target)\n",
        "# In scikit-learn, the feature matrix (X) needs to be 2D, e.g., using .reshape(-1, 1)\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "y = np.array([2, 3, 5, 4, 6])\n",
        "\n",
        "# 2. Create a linear regression model instance\n",
        "model = LinearRegression()\n",
        "\n",
        "# 3. Fit the model to the data\n",
        "model.fit(X, y)\n",
        "\n",
        "# 4. Print the slope (coefficient) and intercept\n",
        "# The slope is stored in model.coef_ and the intercept in model.intercept_\n",
        "print(f\"Slope (Coefficient): {model.coef_[0]}\")\n",
        "print(f\"Intercept: {model.intercept_}\")\n",
        "\n",
        "# Optional: Predict a new value (e.g., for X=6)\n",
        "# new_data = np.array([6]).reshape(-1, 1)\n",
        "# prediction = model.predict(new_data)\n",
        "# print(f\"Prediction for X=6: {prediction[0]}\")\n"
      ],
      "metadata": {
        "id": "LNMgPglurDsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. question\n",
        "\n",
        "In a simple linear regression (\\(y=\\beta _{0}+\\beta _{1}x+\\epsilon \\)), the slope coefficient (\\(\\beta _{1}\\)) shows the estimated change in the dependent variable (\\(y\\)) for a one-unit increase in the independent variable (\\(x\\)), while the intercept (\\(\\beta _{0}\\)) is the predicted value of \\(y\\) when \\(x\\) is zero, though its meaningfulness depends on whether \\(x=0\\) is a relevant data point. A positive \\(\\beta _{1}\\) means \\(y\\) increases with \\(x\\); a negative \\(\\beta _{1}\\) means \\(y\\) decreases as \\(x\\) increases.\n",
        "\n",
        "* Interpreting the Slope .\n",
        "\n",
        "Positive \\(\\beta _{1}\\): As \\(x\\) goes up by one unit, \\(y\\) goes up by \\(\\beta _{1}\\) units (e.g., more study time, higher test scores).\n",
        "\n",
        "Negative \\(\\beta _{1}\\): As \\(x\\) goes up by one unit, \\(y\\) goes down by \\(|\\beta _{1}|\\) units (e.g., more experience, lower error rate)\n",
        "\n",
        "Negative \\(\\beta _{1}\\): As \\(x\\) goes up by one unit, \\(y\\) goes down by \\(|\\beta _{1}|\\) units (e.g., more experience, lower error rate)"
      ],
      "metadata": {
        "id": "FRE0G-v-rK_p"
      }
    }
  ]
}